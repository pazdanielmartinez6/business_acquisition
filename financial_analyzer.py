"""
Financial Analyzer Module for Boomer Business Finder
Parses financial data, calculates metrics, and generates analysis reports

VERSION: 1.0
"""

import pandas as pd
import json
import numpy as np
from datetime import datetime
import re
from pathlib import Path


class FinancialAnalyzer:
    """
    Analyzes company financial data and generates enhanced reports
    """
    
    def __init__(self, csv_filepath):
        """
        Initialize analyzer with CSV file path
        
        Args:
            csv_filepath: Path to the CSV file generated by BoomerBusinessFinder
        """
        self.csv_filepath = csv_filepath
        self.df = None
        self.enhanced_df = None
        self.top_prospects = None
        self.stats = {}
        
        # Scoring weights for composite score
        self.weights = {
            'asset_size': 0.40,
            'asset_stability': 0.30,
            'data_quality': 0.20,
            'asset_growth': 0.10
        }
    
    def load_data(self):
        """Load CSV file into DataFrame"""
        try:
            self.df = pd.read_csv(self.csv_filepath)
            print(f"✓ Loaded {len(self.df)} companies from {self.csv_filepath}")
            return True
        except Exception as e:
            print(f"✗ Error loading CSV: {str(e)}")
            return False
    
    def parse_financial_data(self, financial_json):
        """
        Parse the Financial_Data JSON column and extract key metrics
        
        Args:
            financial_json: JSON string containing financial data
            
        Returns:
            dict: Parsed financial metrics
        """
        result = {
            'current_assets_data': [],
            'fixed_assets_data': [],
            'has_data': False,
            'data_status': 'No_Financial_Data'
        }
        
        # Handle N/A or missing data
        if pd.isna(financial_json) or financial_json == 'N/A':
            return result
        
        try:
            data = json.loads(financial_json)
            
            # Extract from financial_summary (most common location)
            fin_summary = data.get('financial_summary', {})
            
            # Get current assets
            current_assets = fin_summary.get('current_assets', [])
            if current_assets and isinstance(current_assets, list):
                result['current_assets_data'] = [
                    {'date': item.get('date'), 'value': item.get('value')}
                    for item in current_assets
                    if isinstance(item, dict)
                ]
            
            # Get fixed assets
            fixed_assets = fin_summary.get('fixed_assets', [])
            if fixed_assets and isinstance(fixed_assets, list):
                result['fixed_assets_data'] = [
                    {'date': item.get('date'), 'value': item.get('value')}
                    for item in fixed_assets
                    if isinstance(item, dict)
                ]
            
            # Determine if we have any data
            has_current = len(result['current_assets_data']) > 0
            has_fixed = len(result['fixed_assets_data']) > 0
            
            if has_current or has_fixed:
                result['has_data'] = True
                result['data_status'] = 'Has_Data'
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"  ⚠ JSON parse error: {str(e)[:100]}")
            return result
        except Exception as e:
            print(f"  ⚠ Unexpected error parsing financial data: {str(e)[:100]}")
            return result
    
    def get_latest_non_null_value(self, asset_data, max_age_years=3):
        """
        Get the most recent non-null asset value
        
        Args:
            asset_data: List of {date, value} dicts
            max_age_years: Maximum age in years for data to be considered fresh
            
        Returns:
            tuple: (value, year, is_stale)
        """
        if not asset_data:
            return None, None, True
        
        # Filter for non-null values and sort by date descending
        valid_data = [
            item for item in asset_data 
            if item.get('value') is not None and item.get('date')
        ]
        
        if not valid_data:
            return None, None, True
        
        # Sort by date (most recent first)
        valid_data_sorted = sorted(
            valid_data, 
            key=lambda x: x['date'], 
            reverse=True
        )
        
        latest = valid_data_sorted[0]
        latest_value = latest['value']
        
        # Convert to float if it's a string
        if isinstance(latest_value, str):
            try:
                latest_value = float(latest_value.replace(',', ''))
            except:
                latest_value = None
        
        latest_year = latest['date'][:4] if latest['date'] else None
        
        # Check if data is stale (older than max_age_years)
        if latest_year:
            current_year = datetime.now().year
            age = current_year - int(latest_year)
            is_stale = age > max_age_years
        else:
            is_stale = True
        
        return latest_value, latest_year, is_stale
    
    def calculate_growth_rate(self, asset_data):
        """
        Calculate asset growth rate over available timespan
        
        Args:
            asset_data: List of {date, value} dicts
            
        Returns:
            tuple: (growth_rate_percent, years_span, status)
        """
        if not asset_data:
            return None, 0, 'Insufficient_Data'
        
        # Filter for non-null values and sort by date
        valid_data = [
            item for item in asset_data 
            if item.get('value') is not None and item.get('date')
        ]
        
        if len(valid_data) < 2:
            return None, len(valid_data), 'Insufficient_Data'
        
        # Sort by date (oldest to newest)
        valid_data_sorted = sorted(valid_data, key=lambda x: x['date'])
        
        oldest = valid_data_sorted[0]
        newest = valid_data_sorted[-1]
        
        oldest_value = oldest['value']
        newest_value = newest['value']
        
        # Convert to float if strings
        if isinstance(oldest_value, str):
            try:
                oldest_value = float(oldest_value.replace(',', ''))
            except:
                return None, len(valid_data), 'Invalid_Base_Value'
        
        if isinstance(newest_value, str):
            try:
                newest_value = float(newest_value.replace(',', ''))
            except:
                return None, len(valid_data), 'Invalid_Base_Value'
        
        # Avoid division by zero
        if oldest_value == 0 or oldest_value is None:
            return None, len(valid_data), 'Invalid_Base_Value'
        
        # Calculate growth rate
        growth_rate = ((newest_value - oldest_value) / oldest_value) * 100
        
        # Calculate time span
        oldest_year = int(oldest['date'][:4])
        newest_year = int(newest['date'][:4])
        years_span = newest_year - oldest_year
        
        return growth_rate, years_span, 'Calculated'
    
    def calculate_stability_score(self, asset_data):
        """
        Calculate asset stability score using Coefficient of Variation
        Lower CV = more stable = higher score
        
        Args:
            asset_data: List of {date, value} dicts
            
        Returns:
            float: Stability score (0-100)
        """
        if not asset_data:
            return 0
        
        # Filter for non-null values
        valid_values = []
        for item in asset_data:
            if item.get('value') is not None:
                val = item['value']
                # Convert to float if string
                if isinstance(val, str):
                    try:
                        val = float(val.replace(',', ''))
                    except:
                        continue
                valid_values.append(val)
        
        if len(valid_values) < 2:
            return 50  # Neutral score if insufficient data
        
        # Calculate coefficient of variation
        mean_value = np.mean(valid_values)
        std_value = np.std(valid_values)
        
        if mean_value == 0:
            return 0
        
        cv = (std_value / mean_value) * 100
        
        # Convert to stability score (100 = perfectly stable, 0 = highly volatile)
        stability_score = max(0, 100 - cv)
        
        return stability_score
    
    def calculate_data_quality_score(self, current_assets_data, fixed_assets_data, 
                                     latest_year_current, latest_year_fixed):
        """
        Calculate data quality score (0-100) based on:
        - Years of data available
        - Percentage of non-null values
        - Recency of latest data
        
        Args:
            current_assets_data: List of current asset entries
            fixed_assets_data: List of fixed asset entries
            latest_year_current: Most recent year for current assets
            latest_year_fixed: Most recent year for fixed assets
            
        Returns:
            float: Data quality score (0-100)
        """
        score = 0
        
        # Component 1: Years of data (40 points max)
        total_entries = len(current_assets_data) + len(fixed_assets_data)
        years_score = min(40, total_entries * 5)  # 5 points per year, max 40
        
        # Component 2: Non-null percentage (30 points max)
        if total_entries > 0:
            current_non_null = sum(1 for item in current_assets_data if item.get('value') is not None)
            fixed_non_null = sum(1 for item in fixed_assets_data if item.get('value') is not None)
            total_non_null = current_non_null + fixed_non_null
            
            non_null_pct = (total_non_null / total_entries) * 100
            non_null_score = (non_null_pct / 100) * 30
        else:
            non_null_score = 0
        
        # Component 3: Recency (30 points max)
        current_year = datetime.now().year
        recency_score = 0
        
        latest_years = []
        if latest_year_current:
            latest_years.append(int(latest_year_current))
        if latest_year_fixed:
            latest_years.append(int(latest_year_fixed))
        
        if latest_years:
            most_recent = max(latest_years)
            age = current_year - most_recent
            
            if age <= 1:
                recency_score = 30
            elif age <= 2:
                recency_score = 25
            elif age <= 3:
                recency_score = 20
            elif age <= 5:
                recency_score = 10
            else:
                recency_score = 5
        
        score = years_score + non_null_score + recency_score
        
        return min(100, score)
    
    def normalize_score(self, value, min_val, max_val):
        """
        Normalize a value to 0-100 scale
        
        Args:
            value: Value to normalize
            min_val: Minimum value in dataset
            max_val: Maximum value in dataset
            
        Returns:
            float: Normalized score (0-100)
        """
        if pd.isna(value) or max_val == min_val:
            return 0
        
        normalized = ((value - min_val) / (max_val - min_val)) * 100
        return max(0, min(100, normalized))
    
    def extract_city_from_address(self, address):
        """
        Extract city/region from registered address
        
        Args:
            address: Full registered address string
            
        Returns:
            str: Extracted city name or 'Unknown'
        """
        if pd.isna(address) or address == 'N/A':
            return 'Unknown'
        
        # Split by comma and take the second-to-last part (typically city)
        parts = [p.strip() for p in address.split(',')]
        
        if len(parts) >= 2:
            # Return second-to-last part (usually city)
            return parts[-2]
        elif len(parts) == 1:
            return parts[0]
        else:
            return 'Unknown'
    
    def extract_primary_sic_code(self, industry_codes_json):
        """
        Extract primary SIC code (first one) from Industry_Codes JSON
        
        Args:
            industry_codes_json: JSON string of industry codes
            
        Returns:
            str: Primary SIC code or 'N/A'
        """
        if pd.isna(industry_codes_json) or industry_codes_json == 'N/A':
            return 'N/A'
        
        try:
            codes = json.loads(industry_codes_json)
            
            if codes and isinstance(codes, list) and len(codes) > 0:
                first_code = codes[0]
                
                # Handle different formats
                if isinstance(first_code, dict):
                    return first_code.get('code', 'N/A')
                elif isinstance(first_code, str):
                    return first_code
                else:
                    return 'N/A'
            else:
                return 'N/A'
                
        except:
            return 'N/A'
    
    def categorize_sic_code(self, industry_category, sic_code):
        """
        Create broader SIC category based on industry category
        
        Args:
            industry_category: Industry category from search
            sic_code: SIC code
            
        Returns:
            str: Broader category
        """
        if pd.isna(industry_category):
            return 'Other'
        
        # Map industry categories to broader groups
        category_map = {
            'Accounting': 'Professional Services',
            'Laundromats': 'Consumer Services',
            'Storage': 'Real Estate & Storage',
            'ATM_Vending': 'Equipment & Services'
        }
        
        return category_map.get(industry_category, 'Other')
    
    def analyze_companies(self):
        """
        Main analysis function - processes all companies and calculates metrics
        """
        if self.df is None:
            print("✗ No data loaded. Call load_data() first.")
            return False
        
        print(f"\n{'='*60}")
        print(f"ANALYZING {len(self.df)} COMPANIES")
        print(f"{'='*60}\n")
        
        # Initialize new columns
        new_columns = {
            'Latest_Current_Assets': [],
            'Latest_Fixed_Assets': [],
            'Total_Assets': [],
            'Current_Fixed_Ratio': [],
            'Asset_Growth_Rate_3yr': [],
            'Asset_Stability_Score': [],
            'Data_Quality_Score': [],
            'Years_of_Data': [],
            'Latest_Data_Year': [],
            'Data_Status': [],
            'City': [],
            'Primary_SIC_Code': [],
            'SIC_Category': []
        }
        
        # Process each company
        for idx, row in self.df.iterrows():
            if (idx + 1) % 10 == 0:
                print(f"  Processing company {idx + 1}/{len(self.df)}...")
            
            # Parse financial data
            fin_data = self.parse_financial_data(row.get('Financial_Data'))
            
            current_assets_data = fin_data['current_assets_data']
            fixed_assets_data = fin_data['fixed_assets_data']
            
            # Get latest values
            latest_current, year_current, stale_current = self.get_latest_non_null_value(
                current_assets_data, max_age_years=3
            )
            latest_fixed, year_fixed, stale_fixed = self.get_latest_non_null_value(
                fixed_assets_data, max_age_years=3
            )
            
            # Calculate total assets
            total_assets = 0
            if latest_current is not None:
                total_assets += latest_current
            if latest_fixed is not None:
                total_assets += latest_fixed
            
            total_assets = total_assets if total_assets > 0 else None
            
            # Calculate current/fixed ratio
            if latest_current is not None and latest_fixed is not None and latest_fixed != 0:
                current_fixed_ratio = latest_current / latest_fixed
            else:
                current_fixed_ratio = None
            
            # Calculate growth rate (use total assets data)
            all_assets_data = current_assets_data + fixed_assets_data
            growth_rate, years_span, growth_status = self.calculate_growth_rate(all_assets_data)
            
            # Calculate stability score
            stability_score = self.calculate_stability_score(all_assets_data)
            
            # Calculate data quality score
            data_quality = self.calculate_data_quality_score(
                current_assets_data, 
                fixed_assets_data,
                year_current,
                year_fixed
            )
            
            # Years of data
            years_of_data = len(set(
                [item['date'][:4] for item in current_assets_data if item.get('date')] +
                [item['date'][:4] for item in fixed_assets_data if item.get('date')]
            ))
            
            # Latest data year
            latest_years = []
            if year_current:
                latest_years.append(year_current)
            if year_fixed:
                latest_years.append(year_fixed)
            
            latest_data_year = max(latest_years) if latest_years else None
            
            # Determine data status
            if not fin_data['has_data']:
                data_status = 'No_Financial_Data'
            elif stale_current and stale_fixed:
                data_status = 'Stale_Data'
            elif (latest_current is None and latest_fixed is None):
                data_status = 'Incomplete_Data'
            else:
                data_status = 'Complete'
            
            # Extract city
            city = self.extract_city_from_address(row.get('Registered_Address'))
            
            # Extract SIC code
            primary_sic = self.extract_primary_sic_code(row.get('Industry_Codes'))
            
            # Categorize SIC
            sic_category = self.categorize_sic_code(
                row.get('Industry_Category'), 
                primary_sic
            )
            
            # Append to new columns
            new_columns['Latest_Current_Assets'].append(latest_current)
            new_columns['Latest_Fixed_Assets'].append(latest_fixed)
            new_columns['Total_Assets'].append(total_assets)
            new_columns['Current_Fixed_Ratio'].append(current_fixed_ratio)
            new_columns['Asset_Growth_Rate_3yr'].append(growth_rate)
            new_columns['Asset_Stability_Score'].append(stability_score)
            new_columns['Data_Quality_Score'].append(data_quality)
            new_columns['Years_of_Data'].append(years_of_data)
            new_columns['Latest_Data_Year'].append(latest_data_year)
            new_columns['Data_Status'].append(data_status)
            new_columns['City'].append(city)
            new_columns['Primary_SIC_Code'].append(primary_sic)
            new_columns['SIC_Category'].append(sic_category)
        
        # Add new columns to dataframe
        for col_name, col_data in new_columns.items():
            self.df[col_name] = col_data
        
        print(f"✓ Calculated all financial metrics\n")
        
        # Calculate composite scores
        self.calculate_composite_scores()
        
        self.enhanced_df = self.df.copy()
        
        return True
    
    def calculate_composite_scores(self):
        """
        Calculate composite scores based on weighted metrics
        """
        print("Calculating composite scores...")
        
        # Filter companies with sufficient data for scoring
        scorable = self.df[
            (self.df['Total_Assets'].notna()) & 
            (self.df['Total_Assets'] > 0)
        ].copy()
        
        if len(scorable) == 0:
            print("⚠ No companies with sufficient data for composite scoring")
            self.df['Composite_Score'] = 0
            return
        
        # Normalize each component to 0-100 scale
        
        # 1. Asset Size Score (40%)
        if scorable['Total_Assets'].notna().any():
            min_assets = scorable['Total_Assets'].min()
            max_assets = scorable['Total_Assets'].max()
            scorable['Asset_Size_Score'] = scorable['Total_Assets'].apply(
                lambda x: self.normalize_score(x, min_assets, max_assets)
            )
        else:
            scorable['Asset_Size_Score'] = 0
        
        # 2. Asset Stability Score (30%) - already 0-100
        scorable['Asset_Stability_Score'] = scorable['Asset_Stability_Score'].fillna(0)
        
        # 3. Data Quality Score (20%) - already 0-100
        scorable['Data_Quality_Score'] = scorable['Data_Quality_Score'].fillna(0)
        
        # 4. Asset Growth Score (10%)
        # Normalize growth rate: 0% growth = 50 points, positive growth up to 100, negative down to 0
        def normalize_growth(growth_rate):
            if pd.isna(growth_rate):
                return 50  # Neutral score
            
            # Cap growth at +/-100% for scoring purposes
            capped_growth = max(-100, min(100, growth_rate))
            
            # Convert to 0-100 scale (0% = 50, +100% = 100, -100% = 0)
            score = 50 + (capped_growth / 2)
            
            return max(0, min(100, score))
        
        scorable['Asset_Growth_Score'] = scorable['Asset_Growth_Rate_3yr'].apply(normalize_growth)
        
        # Calculate weighted composite score
        scorable['Composite_Score'] = (
            scorable['Asset_Size_Score'] * self.weights['asset_size'] +
            scorable['Asset_Stability_Score'] * self.weights['asset_stability'] +
            scorable['Data_Quality_Score'] * self.weights['data_quality'] +
            scorable['Asset_Growth_Score'] * self.weights['asset_growth']
        )
        
        # Round to 2 decimal places
        scorable['Composite_Score'] = scorable['Composite_Score'].round(2)
        
        # Merge back to main dataframe
        score_columns = ['Asset_Size_Score', 'Asset_Growth_Score', 'Composite_Score']
        
        for col in score_columns:
            if col not in self.df.columns:
                self.df[col] = 0.0
        
        self.df.update(scorable[score_columns])
        
        # Fill NaN composite scores with 0 for non-scorable companies
        self.df['Composite_Score'] = self.df['Composite_Score'].fillna(0)
        
        print(f"✓ Composite scores calculated for {len(scorable)} companies\n")
    
    def generate_top_prospects(self, min_years=2, min_non_null=1):
        """
        Generate Top 20 prospects table based on composite score
        
        Filters for:
        - At least min_years of data
        - At least min_non_null non-null value for either current or fixed assets
        
        Args:
            min_years: Minimum years of data required
            min_non_null: Minimum non-null entries required
        """
        print("Generating Top 20 prospects...")
        
        # Filter for quality data
        prospects = self.enhanced_df[
            (self.enhanced_df['Years_of_Data'] >= min_years) &
            (
                (self.enhanced_df['Latest_Current_Assets'].notna()) |
                (self.enhanced_df['Latest_Fixed_Assets'].notna())
            ) &
            (self.enhanced_df['Composite_Score'] > 0)
        ].copy()
        
        # Sort by composite score
        prospects = prospects.sort_values('Composite_Score', ascending=False)
        
        # Take top 20
        self.top_prospects = prospects.head(20)
        
        print(f"✓ Top 20 prospects identified (from {len(prospects)} qualified companies)\n")
        
        return self.top_prospects
    
    def generate_statistics(self):
        """
        Generate summary statistics for the report
        """
        print("Calculating summary statistics...")
        
        self.stats = {
            'total_companies': len(self.enhanced_df),
            'data_status_breakdown': self.enhanced_df['Data_Status'].value_counts().to_dict(),
            'avg_total_assets': self.enhanced_df['Total_Assets'].mean(),
            'median_total_assets': self.enhanced_df['Total_Assets'].median(),
            'avg_data_quality': self.enhanced_df['Data_Quality_Score'].mean(),
            'top_10_cities': self.enhanced_df['City'].value_counts().head(10).to_dict(),
            'industry_breakdown': self.enhanced_df['Industry_Category'].value_counts().to_dict(),
            'avg_assets_by_industry': self.enhanced_df.groupby('Industry_Category')['Total_Assets'].mean().to_dict(),
            'composite_score_percentiles': {
                '25th': self.enhanced_df['Composite_Score'].quantile(0.25),
                '50th': self.enhanced_df['Composite_Score'].quantile(0.50),
                '75th': self.enhanced_df['Composite_Score'].quantile(0.75),
                '90th': self.enhanced_df['Composite_Score'].quantile(0.90)
            }
        }
        
        print(f"✓ Statistics calculated\n")
        
        return self.stats
    
    def save_enhanced_csv(self, output_path=None):
        """
        Save enhanced CSV with all calculated columns
        """
        if self.enhanced_df is None:
            print("✗ No enhanced data to save")
            return None
        
        if output_path is None:
            # Generate output path in /mnt/user-data/outputs directory
            input_path = Path(self.csv_filepath)
            output_path = input_path.parent / f"{input_path.stem}_ANALYZED.csv"
        
        try:
            self.enhanced_df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"✓ Enhanced CSV saved: {output_path}")
            return str(output_path)
        except Exception as e:
            print(f"✗ Error saving enhanced CSV: {str(e)}")
            return None
    
    def save_top_prospects_csv(self, output_path=None):
        """
        Save Top 20 prospects CSV
        """
        if self.top_prospects is None or len(self.top_prospects) == 0:
            print("✗ No top prospects to save")
            return None
        
        if output_path is None:
            # Generate output path in /mnt/user-data/outputs directory
            input_path = Path(self.csv_filepath)
            output_path = input_path.parent / f"{input_path.stem}_TOP20.csv"
        
        try:
            # Select relevant columns for prospects
            prospect_columns = [
                'Company_Name',
                'Composite_Score',
                'Total_Assets',
                'Asset_Growth_Rate_3yr',
                'Asset_Stability_Score',
                'Data_Quality_Score',
                'Industry_Category',
                'City',
                'Incorporation_Date',
                'Latest_Data_Year',
                'Years_of_Data',
                'Registered_Address',
                'Company_Number',
                'OpenCorporates_URL'
            ]
            
            # Filter for columns that exist
            available_columns = [col for col in prospect_columns if col in self.top_prospects.columns]
            
            self.top_prospects[available_columns].to_csv(output_path, index=False, encoding='utf-8')
            print(f"✓ Top 20 prospects CSV saved: {output_path}")
            return str(output_path)
        except Exception as e:
            print(f"✗ Error saving top prospects CSV: {str(e)}")
            return None
    
    def save_summary_report(self, output_path=None):
        """
        Save summary statistics report as text file
        """
        if not self.stats:
            print("✗ No statistics to save")
            return None
        
        if output_path is None:
            # Generate output path in /mnt/user-data/outputs directory
            input_path = Path(self.csv_filepath)
            output_path = input_path.parent / f"{input_path.stem}_SUMMARY.txt"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("="*70 + "\n")
                f.write("BOOMER BUSINESS FINDER - FINANCIAL ANALYSIS SUMMARY\n")
                f.write("="*70 + "\n\n")
                
                f.write(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Source File: {self.csv_filepath}\n\n")
                
                # Overview
                f.write("OVERVIEW\n")
                f.write("-"*70 + "\n")
                f.write(f"Total Companies Analyzed: {self.stats['total_companies']}\n\n")
                
                # Data Status Breakdown
                f.write("DATA STATUS BREAKDOWN\n")
                f.write("-"*70 + "\n")
                for status, count in self.stats['data_status_breakdown'].items():
                    pct = (count / self.stats['total_companies']) * 100
                    f.write(f"  {status}: {count} ({pct:.1f}%)\n")
                f.write("\n")
                
                # Financial Metrics
                f.write("FINANCIAL METRICS\n")
                f.write("-"*70 + "\n")
                
                if not pd.isna(self.stats['avg_total_assets']):
                    f.write(f"  Average Total Assets: £{self.stats['avg_total_assets']:,.0f}\n")
                else:
                    f.write(f"  Average Total Assets: N/A\n")
                
                if not pd.isna(self.stats['median_total_assets']):
                    f.write(f"  Median Total Assets: £{self.stats['median_total_assets']:,.0f}\n")
                else:
                    f.write(f"  Median Total Assets: N/A\n")
                
                f.write(f"  Average Data Quality Score: {self.stats['avg_data_quality']:.1f}/100\n\n")
                
                # Top 10 Cities
                f.write("TOP 10 CITIES (by company count)\n")
                f.write("-"*70 + "\n")
                for idx, (city, count) in enumerate(self.stats['top_10_cities'].items(), 1):
                    f.write(f"  {idx}. {city}: {count} companies\n")
                f.write("\n")
                
                # Average Assets by Industry
                f.write("AVERAGE ASSET SIZE BY INDUSTRY\n")
                f.write("-"*70 + "\n")
                for industry, avg_assets in self.stats['avg_assets_by_industry'].items():
                    if not pd.isna(avg_assets):
                        f.write(f"  {industry}: £{avg_assets:,.0f}\n")
                    else:
                        f.write(f"  {industry}: N/A\n")
                f.write("\n")
                
                # Composite Score Distribution
                f.write("COMPOSITE SCORE DISTRIBUTION\n")
                f.write("-"*70 + "\n")
                percentiles = self.stats['composite_score_percentiles']
                f.write(f"  25th Percentile: {percentiles['25th']:.2f}\n")
                f.write(f"  50th Percentile (Median): {percentiles['50th']:.2f}\n")
                f.write(f"  75th Percentile: {percentiles['75th']:.2f}\n")
                f.write(f"  90th Percentile: {percentiles['90th']:.2f}\n\n")
                
                # Top 20 Companies
                if self.top_prospects is not None and len(self.top_prospects) > 0:
                    f.write("TOP 20 COMPANIES (by Composite Score)\n")
                    f.write("="*70 + "\n\n")
                    
                    for idx, row in self.top_prospects.iterrows():
                        rank = self.top_prospects.index.get_loc(idx) + 1
                        f.write(f"{rank}. {row['Company_Name']}\n")
                        f.write(f"   Composite Score: {row['Composite_Score']:.2f}/100\n")
                        
                        if not pd.isna(row['Total_Assets']):
                            f.write(f"   Total Assets: £{row['Total_Assets']:,.0f}\n")
                        
                        if not pd.isna(row['Asset_Growth_Rate_3yr']):
                            f.write(f"   3-Year Growth Rate: {row['Asset_Growth_Rate_3yr']:.1f}%\n")
                        
                        f.write(f"   Data Quality Score: {row['Data_Quality_Score']:.1f}/100\n")
                        f.write(f"   Industry: {row['Industry_Category']}\n")
                        f.write(f"   Location: {row['City']}\n")
                        f.write(f"   Years of Data: {row['Years_of_Data']}\n")
                        f.write("\n")
                
                f.write("="*70 + "\n")
                f.write("END OF REPORT\n")
                f.write("="*70 + "\n")
            
            print(f"✓ Summary report saved: {output_path}")
            return str(output_path)
            
        except Exception as e:
            print(f"✗ Error saving summary report: {str(e)}")
            return None
    
    def run_full_analysis(self):
        """
        Execute complete analysis pipeline
        
        Returns:
            dict: Paths to all generated files
        """
        print("\n" + "="*70)
        print("FINANCIAL ANALYZER - STARTING FULL ANALYSIS")
        print("="*70 + "\n")
        
        # Load data
        if not self.load_data():
            return None
        
        # Analyze companies
        if not self.analyze_companies():
            return None
        
        # Generate top prospects
        self.generate_top_prospects()
        
        # Generate statistics
        self.generate_statistics()
        
        # Save all outputs
        print("\n" + "="*70)
        print("SAVING OUTPUTS")
        print("="*70 + "\n")
        
        enhanced_csv_path = self.save_enhanced_csv()
        top20_csv_path = self.save_top_prospects_csv()
        summary_path = self.save_summary_report()
        
        print("\n" + "="*70)
        print("✓ ANALYSIS COMPLETE!")
        print("="*70)
        print(f"\nGenerated Files:")
        print(f"  1. {enhanced_csv_path}")
        print(f"  2. {top20_csv_path}")
        print(f"  3. {summary_path}")
        print("="*70 + "\n")
        
        return {
            'enhanced_csv': enhanced_csv_path,
            'top_prospects_csv': top20_csv_path,
            'summary_report': summary_path
        }


def analyze_results(csv_filepath):
    """
    Convenience function to run analysis on a CSV file
    
    Args:
        csv_filepath: Path to CSV file from BoomerBusinessFinder
        
    Returns:
        dict: Paths to generated files
    """
    analyzer = FinancialAnalyzer(csv_filepath)
    return analyzer.run_full_analysis()


if __name__ == "__main__":
    """
    Standalone execution mode
    """
    import sys
    
    if len(sys.argv) < 2:
        print("\n" + "="*70)
        print("FINANCIAL ANALYZER - Standalone Mode")
        print("="*70)
        print("\nUsage: python financial_analyzer.py <csv_filepath>")
        print("\nExample:")
        print("  python financial_analyzer.py boomer_businesses_v2_1_TEST_20251127_182056.csv")
        print("="*70 + "\n")
        sys.exit(1)
    
    csv_filepath = sys.argv[1]
    
    # Check if file exists
    if not Path(csv_filepath).exists():
        print(f"\n✗ Error: File not found: {csv_filepath}\n")
        sys.exit(1)
    
    # Run analysis
    analyze_results(csv_filepath)
